{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T14:52:26.529913Z",
     "start_time": "2024-05-02T14:52:26.528046Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from gensim.models import FastText\n",
    "from keras.layers import LSTM, Dense, Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T14:52:26.584815Z",
     "start_time": "2024-05-02T14:52:26.533841Z"
    }
   },
   "source": [
    "sentimen_data = pd.read_csv('../data/Indonesian Sentiment Twitter Dataset Labeled.csv', sep=\"\\t\")\n",
    "sentimen_data.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "source": [
    "alay_dict = pd.read_csv(\"../data/new_kamusalay.csv\", encoding = 'latin-1', header=None)\n",
    "alay_dict = alay_dict.rename(columns=  {0: 'original',\n",
    "                                        1: 'replacement'})\n",
    "stopword_dict = pd.read_csv('../data/stopwordbahasa.csv', header=None)\n",
    "stopword_dict = stopword_dict.rename(columns={0: 'stopword'})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T14:52:26.680731Z",
     "start_time": "2024-05-02T14:52:26.578942Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T14:52:26.682115Z",
     "start_time": "2024-05-02T14:52:26.631424Z"
    }
   },
   "source": [
    "sentimen_data.isnull().sum()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T14:52:26.752671Z",
     "start_time": "2024-05-02T14:52:26.642820Z"
    }
   },
   "source": [
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "#lowercase\n",
    "def lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_unnecessary_char(text):\n",
    "    text = re.sub('\\n',' ',text) # Remove every '\\n'\n",
    "    text = re.sub('rt',' ',text) # Remove every retweet symbol\n",
    "    text = re.sub('((@[^\\s]+)|(#[^\\s]+))',' ',text)\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))',' ',text) # Remove every URL\n",
    "    text = re.sub('  +', ' ', text) # Remove extra spaces\n",
    "    return text\n",
    "    \n",
    "def remove_nonaplhanumeric(text):\n",
    "    text = re.sub('[^0-9a-zA-Z]+', ' ', text) \n",
    "    return text\n",
    "\n",
    "alay_dict_map = dict(zip(alay_dict['original'], alay_dict['replacement']))\n",
    "def normalize_alay(text):\n",
    "    return ' '.join([alay_dict_map[word] if word in alay_dict_map else word for word in text.split(' ')])\n",
    "\n",
    "def remove_stopword(text):\n",
    "    text = ' '.join(['' if word in stopword_dict.stopword.values else word for word in text.split(' ')])\n",
    "    text = re.sub('  +', ' ', text) # Remove extra spaces\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def stemming(text):\n",
    "    return stemmer.stem(text)\n",
    "\n",
    "def preprocess(text):\n",
    "    text = lowercase(text) # 1\n",
    "    text = remove_unnecessary_char(text) # 2\n",
    "    text = remove_nonaplhanumeric(text) # 3\n",
    "    text = normalize_alay(text) # 4\n",
    "    text = remove_stopword(text) # 5\n",
    "    text = stemming(text) # 6\n",
    "    return text"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T14:52:26.753423Z",
     "start_time": "2024-05-02T14:52:26.717680Z"
    }
   },
   "source": [
    "sentimen_data.replace(-1, 0, inplace=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "source": [
    "X = sentimen_data['Tweet'].apply(preprocess)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T14:52:31.518365Z",
     "start_time": "2024-05-02T14:52:26.820921Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T14:52:31.551777Z",
     "start_time": "2024-05-02T14:52:31.517899Z"
    }
   },
   "source": [
    "sentimen_data.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T14:52:31.557022Z",
     "start_time": "2024-05-02T14:52:31.551651Z"
    }
   },
   "source": [
    "tweets = sentimen_data['Tweet'].values\n",
    "sentiments = sentimen_data['sentimen'].values"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "source": [
    "max_features = 1000\n",
    "max_len=50\n",
    "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
    "tokenizer.fit_on_texts(X.values)\n",
    "X = tokenizer.texts_to_sequences(X.values)\n",
    "X = pad_sequences(X, padding='post', maxlen=max_len)\n",
    "X.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T14:52:31.750478Z",
     "start_time": "2024-05-02T14:52:31.602048Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "source": [
    "sentiment_encode = {-1 : 0, 0 : 1, 1 : 2}\n",
    "y = sentimen_data['sentimen'].map(sentiment_encode).values\n",
    "print(sentimen_data['sentimen'])\n",
    "print(y)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T14:52:31.751693Z",
     "start_time": "2024-05-02T14:52:31.747714Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15, random_state = 1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.20, random_state=1)\n",
    "y_train = to_categorical(y_train, 3)\n",
    "y_test = to_categorical(y_test, 3)\n",
    "y_val = to_categorical(y_val, 3)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T14:52:31.774754Z",
     "start_time": "2024-05-02T14:52:31.752008Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T14:52:58.901475Z",
     "start_time": "2024-05-02T14:52:31.819494Z"
    }
   },
   "source": [
    "# Tokenize text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(tweets)\n",
    "sequences = tokenizer.texts_to_sequences(tweets)\n",
    "\n",
    "# Pad sequences\n",
    "max_seq_length = max(len(x) for x in sequences)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_seq_length)\n",
    "\n",
    "# Train FastText\n",
    "model_ft = FastText([seq.split() for seq in tweets], vector_size=150, window=5, min_count=3, epochs=100)\n",
    "\n",
    "# Create embedding matrix\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_matrix = np.zeros((vocab_size, 150))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    try:\n",
    "        embedding_vector = model_ft.wv[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        continue\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T14:54:53.552058Z",
     "start_time": "2024-05-02T14:52:58.925520Z"
    }
   },
   "source": [
    "# Build LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=150, weights=[embedding_matrix], input_length=max_seq_length, trainable=False))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "epochs=1\n",
    "\n",
    "# Convert sentiments to binary labels\n",
    "binary_labels = np.array([1 if x == 1 else 0 for x in sentiments])\n",
    "\n",
    "# Train model\n",
    "history = model.fit(padded_sequences, binary_labels, epochs=epochs, batch_size=1)\n",
    "\n",
    "# batch_size = 64\n",
    "# model.fit(X_train, y_train,\n",
    "#                       validation_data=(X_val, y_val),\n",
    "#                       batch_size=batch_size, epochs=epochs, verbose=2)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "source": [
    "fig, ax = plt.subplots(figsize = (10, 4))\n",
    "ax.plot(history.history['accuracy'], label = 'train accuracy')\n",
    "ax.plot(history.history['val_accuracy'], label = 'val accuracy')\n",
    "ax.set_title('Model Accuracy')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.legend(loc = 'upper left')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T14:54:53.799363Z",
     "start_time": "2024-05-02T14:54:53.552358Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-02T14:54:53.782984Z"
    }
   },
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
