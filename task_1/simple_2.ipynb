{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "import re\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from gensim.models import FastText\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T06:05:36.345044Z",
     "start_time": "2024-05-03T06:05:36.344096Z"
    }
   },
   "id": "44168063c65faead"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "   sentimen                                              Tweet\n0        -1  lagu bosan apa yang aku save ni huhuhuhuhuhuhu...\n1        -1  kita lanjutkan saja diam ini hingga kau dan ak...\n2         1  doa rezeki tak putus inna haa zaa larizquna ma...\n3         1  makasih loh ntar kita bagi hasil aku 99 9 sisa...\n4        -1  aku tak faham betul jenis orang malaysia yang ...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentimen</th>\n      <th>Tweet</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-1</td>\n      <td>lagu bosan apa yang aku save ni huhuhuhuhuhuhu...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-1</td>\n      <td>kita lanjutkan saja diam ini hingga kau dan ak...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>doa rezeki tak putus inna haa zaa larizquna ma...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>makasih loh ntar kita bagi hasil aku 99 9 sisa...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-1</td>\n      <td>aku tak faham betul jenis orang malaysia yang ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/Indonesian Sentiment Twitter Dataset Labeled.csv', sep=\"\\t\")\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T06:02:56.690558Z",
     "start_time": "2024-05-03T06:02:56.657437Z"
    }
   },
   "id": "838f30dee040f3d"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "alay_dict = pd.read_csv(\"../data/new_kamusalay.csv\", encoding = 'latin-1', header=None)\n",
    "alay_dict = alay_dict.rename(columns=  {0: 'original',\n",
    "                                        1: 'replacement'})\n",
    "stopword_dict = pd.read_csv('../data/stopwordbahasa.csv', header=None)\n",
    "stopword_dict = stopword_dict.rename(columns={0: 'stopword'})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T06:02:57.755026Z",
     "start_time": "2024-05-03T06:02:57.733596Z"
    }
   },
   "id": "57da99c799483137"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "sentimen    0\nTweet       0\ndtype: int64"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T06:02:58.804548Z",
     "start_time": "2024-05-03T06:02:58.796886Z"
    }
   },
   "id": "757cc68b9087b865"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "#lowercase\n",
    "def lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_unnecessary_char(text):\n",
    "    text = re.sub('\\n',' ',text) # Remove every '\\n'\n",
    "    text = re.sub('rt',' ',text) # Remove every retweet symbol\n",
    "    text = re.sub('((@[^\\s]+)|(#[^\\s]+))',' ',text)\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))',' ',text) # Remove every URL\n",
    "    text = re.sub('  +', ' ', text) # Remove extra spaces\n",
    "    return text\n",
    "    \n",
    "def remove_nonaplhanumeric(text):\n",
    "    text = re.sub('[^0-9a-zA-Z]+', ' ', text) \n",
    "    return text\n",
    "\n",
    "alay_dict_map = dict(zip(alay_dict['original'], alay_dict['replacement']))\n",
    "def normalize_alay(text):\n",
    "    return ' '.join([alay_dict_map[word] if word in alay_dict_map else word for word in text.split(' ')])\n",
    "\n",
    "def remove_stopword(text):\n",
    "    text = ' '.join(['' if word in stopword_dict.stopword.values else word for word in text.split(' ')])\n",
    "    text = re.sub('  +', ' ', text) # Remove extra spaces\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def stemming(text):\n",
    "    return stemmer.stem(text)\n",
    "\n",
    "def preprocess(text):\n",
    "    text = lowercase(text) # 1\n",
    "    text = remove_unnecessary_char(text) # 2\n",
    "    text = remove_nonaplhanumeric(text) # 3\n",
    "    text = normalize_alay(text) # 4\n",
    "    text = remove_stopword(text) # 5\n",
    "    text = stemming(text) # 6\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T06:02:59.815315Z",
     "start_time": "2024-05-03T06:02:59.792648Z"
    }
   },
   "id": "30626513762d8bf2"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "df.replace(-1, 0, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T06:03:05.710715Z",
     "start_time": "2024-05-03T06:03:05.705499Z"
    }
   },
   "id": "cd621908ab347c77"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "X = df['Tweet'].apply(preprocess)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T06:03:11.329931Z",
     "start_time": "2024-05-03T06:03:06.619921Z"
    }
   },
   "id": "a9617a9a8a7d9a3e"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Assuming the CSV file has a column named 'text' that contains the text of each document\n",
    "corpus = X.tolist()\n",
    "sentiments = df['sentimen'].values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T06:03:17.146063Z",
     "start_time": "2024-05-03T06:03:17.141823Z"
    }
   },
   "id": "93f93eb634b06b9b"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "(10806, 50)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_features = 1000\n",
    "max_len=50\n",
    "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
    "tokenizer.fit_on_texts(X.values)\n",
    "X = tokenizer.texts_to_sequences(X.values)\n",
    "X = pad_sequences(X, padding='post', maxlen=max_len)\n",
    "X.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T06:03:17.989715Z",
     "start_time": "2024-05-03T06:03:17.887677Z"
    }
   },
   "id": "277c5dbdb1bd0e26"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        0\n",
      "1        0\n",
      "2        1\n",
      "3        1\n",
      "4        0\n",
      "        ..\n",
      "10801    1\n",
      "10802    0\n",
      "10803    1\n",
      "10804    1\n",
      "10805    1\n",
      "Name: sentimen, Length: 10806, dtype: int64\n",
      "[1 1 2 ... 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "sentiment_encode = {-1 : 0, 0 : 1, 1 : 2}\n",
    "y = df['sentimen'].map(sentiment_encode).values\n",
    "print(df['sentimen'])\n",
    "print(y)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T06:03:18.879192Z",
     "start_time": "2024-05-03T06:03:18.869944Z"
    }
   },
   "id": "5dcac57c1672caba"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7348, 50) (7348, 3)\n",
      "(1621, 50) (1621, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15, random_state = 1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.20, random_state=1)\n",
    "y_train = to_categorical(y_train, 3)\n",
    "y_test = to_categorical(y_test, 3)\n",
    "y_val = to_categorical(y_val, 3)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T06:03:19.650930Z",
     "start_time": "2024-05-03T06:03:19.631656Z"
    }
   },
   "id": "bc64a92f50d4c28c"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# Convert sentiments to binary labels\n",
    "binary_labels = np.array([1 if x == 1 else 0 for x in sentiments])\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the vectorizer on the corpus\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Get the feature names (words)\n",
    "feature_names = vectorizer.get_feature_names_out()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T06:03:20.808178Z",
     "start_time": "2024-05-03T06:03:20.767087Z"
    }
   },
   "id": "35732c7bf6e5e6f"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix:\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "Feature Names:\n",
      "['00' '000' '000ampe' ... 'zulkifli' 'zuragan' 'zzz']\n"
     ]
    }
   ],
   "source": [
    "# Print TF-IDF matrix and feature names\n",
    "print(\"TF-IDF Matrix:\")\n",
    "print(tfidf_matrix.toarray())\n",
    "print(\"\\nFeature Names:\")\n",
    "print(feature_names)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T06:03:21.826229Z",
     "start_time": "2024-05-03T06:03:21.558939Z"
    }
   },
   "id": "6aca21eac798f4d4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fast Text"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e148f5c643bed069"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.8773283e+00 -5.6173515e-01  5.0368216e-02  7.9887342e-01\n",
      "  2.1571274e+00 -1.9283055e-01  8.1208950e-01 -5.3469980e-01\n",
      "  2.9478994e-01 -1.4550249e-01 -1.2917448e+00 -2.8188163e-01\n",
      " -1.3942964e+00  6.0440588e-01 -1.2799377e+00 -2.2690591e-01\n",
      "  1.3942947e+00 -6.5789783e-01 -1.0300760e+00 -2.0422659e+00\n",
      "  7.2681054e-02 -8.2481004e-02 -1.6583884e+00  3.8017708e-01\n",
      " -1.8659068e+00 -1.7409097e-01  6.1934799e-01  8.2431084e-01\n",
      " -9.7448117e-01  3.7246206e-01 -8.0249459e-01  3.5207224e-01\n",
      "  5.8377892e-01 -7.2377008e-01 -6.2820353e-02  6.3628823e-01\n",
      "  4.7861558e-01 -1.1279988e-01 -1.3395792e+00 -5.3948700e-01\n",
      "  1.9469628e-01 -2.5458405e+00 -2.8755498e-01  5.4065841e-01\n",
      " -8.4550664e-02 -5.1059987e-02 -2.2207949e-01 -8.1495178e-01\n",
      " -9.1826133e-02 -9.7936332e-01  4.5537513e-01 -2.4295153e-01\n",
      "  1.3331388e-03  4.4800773e-01  2.5315753e-01 -1.4656006e+00\n",
      "  7.2339021e-02  7.9195686e-02  8.2358187e-01 -1.0866533e-01\n",
      "  9.8380250e-01  5.9673309e-01 -1.1589496e+00  1.3924528e+00\n",
      "  8.0436367e-01  1.6199100e+00  6.1026919e-01  3.0654955e-01\n",
      "  8.0529392e-01  2.0434551e+00  9.7646934e-01  2.6455593e+00\n",
      "  5.3061968e-01 -1.9723794e+00  1.4330193e-01 -5.5107105e-01\n",
      " -3.1020373e-01 -1.1970361e+00 -5.3085905e-01  1.4078237e+00\n",
      "  9.9559176e-01 -1.3936976e+00  4.1347769e-01  1.3505329e-01\n",
      " -8.0825645e-01  1.0657792e+00  1.5629078e-01  5.6001109e-01\n",
      "  4.2142242e-02 -3.9581265e-02 -6.4604145e-01  7.1629161e-01\n",
      "  1.1340693e+00 -2.2540550e-01 -8.7626225e-01  1.2316355e+00\n",
      " -6.0391617e-01 -4.3541434e-01 -9.1575474e-01  1.7915175e+00]\n"
     ]
    }
   ],
   "source": [
    "# Convert the TF-IDF matrix to a dense array\n",
    "dense_tfidf_matrix = tfidf_matrix.toarray()\n",
    "\n",
    "# Initialize a FastText model\n",
    "model_ft = FastText(vector_size=100, window=5, min_count=1)\n",
    "\n",
    "# Build vocabulary and train the FastText model\n",
    "model_ft.build_vocab(corpus_iterable=[text.split() for text in corpus])\n",
    "model_ft.train(corpus_iterable=[text.split() for text in corpus], total_examples=len(corpus), epochs=10)\n",
    "\n",
    "# Initialize an empty matrix to store word embeddings\n",
    "word_embeddings = np.zeros((len(feature_names), model_ft.vector_size))\n",
    "\n",
    "# Map each word to its FastText vector\n",
    "for idx, word in enumerate(feature_names):\n",
    "    if word in model_ft.wv:\n",
    "        word_embeddings[idx] = model_ft.wv[word]\n",
    "\n",
    "word_embed_ft = model_ft.wv[\"dan\"]\n",
    "print(word_embed_ft)\n",
    "\n",
    "# Print word embeddings for the first few words\n",
    "# print(\"Word Embeddings:\")\n",
    "# print(word_embeddings[:10])  # Print embeddings for the first 10 words"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T06:20:11.405569Z",
     "start_time": "2024-05-03T06:20:08.206771Z"
    }
   },
   "id": "d9e381a054122e27"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Glove"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6232781198200f92"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.028669 -0.079521 -0.745437  0.947957  0.1456    0.208793  0.563256\n",
      "  0.144865 -1.168601 -0.770691 -0.868666 -0.099608 -0.677246  0.276379\n",
      " -0.202523 -0.938004  0.869196  1.154606 -0.711352  1.530791  0.633017\n",
      "  1.103734  3.651248 -0.063392 -1.85838   1.355643 -0.342743 -0.638605\n",
      "  0.393076 -1.268856  1.055029 -0.375233 -1.428337  0.428032  1.361086\n",
      "  0.711733  1.206595  1.210484 -0.190727  0.194155  0.26984  -0.191284\n",
      " -0.169478 -0.336127 -0.449392  0.552006 -0.037513 -0.216293  0.687252\n",
      " -0.775978]\n"
     ]
    }
   ],
   "source": [
    "# Load the GloVe word vectors\n",
    "glove_file = '../data/vectors.txt'\n",
    "word_vectors = KeyedVectors.load_word2vec_format(glove_file, binary=False, no_header=True)\n",
    "\n",
    "# Initialize an empty matrix to store word embeddings\n",
    "word_embeddings = np.zeros((len(feature_names), word_vectors.vector_size))\n",
    "\n",
    "# Map each word to its GloVe vector\n",
    "for idx, word in enumerate(feature_names):\n",
    "    if word in word_vectors:\n",
    "        word_embeddings[idx] = word_vectors[word]\n",
    "        \n",
    "word_embed_glove = word_vectors.word_vec('dan')\n",
    "print(word_embed_glove)\n",
    "\n",
    "# Print word embeddings for the first few words\n",
    "# print(\"Word Embeddings:\")\n",
    "# print(word_embeddings[:10])  # Print embeddings for the first 10 words"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T06:22:33.537259Z",
     "start_time": "2024-05-03T06:22:27.250424Z"
    }
   },
   "id": "4322a00ac3a14940"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Word2vec- Cbow"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e70a127c4c585b5d"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tok', 0.9949443340301514), ('low', 0.9947314262390137), ('bukit', 0.9947167634963989), ('taman', 0.9946802258491516), ('ro', 0.9945077896118164), ('burung', 0.9943517446517944), ('iklan', 0.9942890405654907), ('wani', 0.9942115545272827), ('guru', 0.9941952228546143), ('ketjup', 0.9941721558570862)]\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the corpus for Word2Vec\n",
    "tokenized_corpus = [text.split() for text in corpus]\n",
    "\n",
    "# Initialize a Word2Vec CBOW model\n",
    "model_w2v_cbow = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, sg=0, min_count=1, workers=4)\n",
    "\n",
    "# Train the Word2Vec CBOW model\n",
    "model_w2v_cbow.train(tokenized_corpus, total_examples=len(tokenized_corpus), epochs=10)\n",
    "\n",
    "# Initialize an empty matrix to store word embeddings\n",
    "word_embeddings = np.zeros((len(feature_names), model_w2v_cbow.vector_size))\n",
    "\n",
    "# Map each word to its Word2Vec CBOW vector\n",
    "for idx, word in enumerate(feature_names):\n",
    "    if word in model_w2v_cbow.wv:\n",
    "        word_embeddings[idx] = model_w2v_cbow.wv[word]\n",
    "\n",
    "word_embed_cbow = model_w2v_cbow.wv.most_similar('dan')\n",
    "print(word_embed_cbow)\n",
    "\n",
    "# Print word embeddings for the first few words\n",
    "# print(\"Word Embeddings:\")\n",
    "# print(word_embeddings[:10])  # Print embeddings for the first 10 words"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T06:25:58.549681Z",
     "start_time": "2024-05-03T06:25:57.505992Z"
    }
   },
   "id": "4876b39f65c209c2"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('musketeers', 0.9901020526885986), ('indomi', 0.9891538619995117), ('disamperin', 0.9886239767074585), ('dft', 0.9883655309677124), ('tahhh', 0.9881646633148193), ('ditakoni', 0.98738032579422), ('1962aat', 0.9865550398826599), ('cingkrang', 0.9863256216049194), ('tahniah', 0.9862285256385803), ('lban', 0.9862149953842163)]\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the corpus for Word2Vec\n",
    "tokenized_corpus = [text.split() for text in corpus]\n",
    "\n",
    "# Initialize a Word2Vec Skip-gram model\n",
    "model_w2v_sg = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, sg=1, min_count=1, workers=4)\n",
    "\n",
    "# Train the Word2Vec Skip-gram model\n",
    "model_w2v_sg.train(tokenized_corpus, total_examples=len(tokenized_corpus), epochs=10)\n",
    "\n",
    "# Initialize an empty matrix to store word embeddings\n",
    "word_embeddings = np.zeros((len(feature_names), model_w2v_sg.vector_size))\n",
    "\n",
    "# Map each word to its Word2Vec Skip-gram vector\n",
    "for idx, word in enumerate(feature_names):\n",
    "    if word in model_w2v_sg.wv:\n",
    "        word_embeddings[idx] = model_w2v_sg.wv[word]\n",
    "        \n",
    "word_embed_skipgram = model_w2v_sg.wv.most_similar('dan')\n",
    "print(word_embed_skipgram)\n",
    "\n",
    "# Print word embeddings for the first few words\n",
    "# print(\"Word Embeddings:\")\n",
    "# print(word_embeddings[:10])  # Print embeddings for the first 10 words"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T06:26:40.516659Z",
     "start_time": "2024-05-03T06:26:38.652940Z"
    }
   },
   "id": "35edf9ab6bc6a900"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e4a87798d6dd57b7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
